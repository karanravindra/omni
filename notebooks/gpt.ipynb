{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5aaec108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchinfo import summary\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a434a8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 96\n",
      "Train tokens: 5,875,117\n",
      "Validation tokens: 652,791\n"
     ]
    }
   ],
   "source": [
    "data = \"\\n\\n\\n\".join(\n",
    "    [\n",
    "        json.loads(line)[\"story\"]\n",
    "        for line in open(\"../data/stories.jsonl\", \"r\").readlines()\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: str,\n",
    "        initial_vocab: Optional[list[str]] = None,\n",
    "        swaps: Optional[dict[str, str]] = None,\n",
    "    ):\n",
    "        if swaps is None:\n",
    "            swaps = {}\n",
    "        for k, v in swaps.items():\n",
    "            data = data.replace(k, v)\n",
    "\n",
    "        if initial_vocab is None:\n",
    "            unique_chars = sorted(set(data))\n",
    "        else:\n",
    "            unique_chars = sorted(set(initial_vocab).union(set(data)))\n",
    "\n",
    "        self.swaps = swaps\n",
    "\n",
    "        self.char2idx = {ch: idx for idx, ch in enumerate(unique_chars)}\n",
    "        self.idx2char = {idx: ch for ch, idx in self.char2idx.items()}\n",
    "\n",
    "    def encode(self, text: str) -> torch.Tensor:\n",
    "        for k, v in self.swaps.items():\n",
    "            text = text.replace(k, v)\n",
    "        return torch.tensor([self.char2idx[ch] for ch in text], dtype=torch.long)\n",
    "\n",
    "    def decode(self, indices: torch.Tensor) -> str:\n",
    "        return \"\".join([self.idx2char[int(idx.item())] for idx in indices])\n",
    "\n",
    "\n",
    "train_data = data[: int(0.9 * len(data))]\n",
    "val_data = data[int(0.9 * len(data)) :]\n",
    "\n",
    "tokenizer = Tokenizer(\n",
    "    train_data,\n",
    "    initial_vocab=[chr(i) for i in range(32, 127)],\n",
    "    swaps={\"é\": \"e\", \"–\": \"-\", \"—\": \"-\", \"’\": \"'\"},\n",
    ")\n",
    "vocab_size = len(tokenizer.char2idx)\n",
    "train_tokens = tokenizer.encode(train_data)\n",
    "val_tokens = tokenizer.encode(val_data)\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Train tokens: {len(train_tokens):,}\")\n",
    "print(f\"Validation tokens: {len(val_tokens):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a24a5394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roundTo32(x, base=32):\n",
    "    return int(base * math.ceil(float(x) / base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01575c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "======================================================================\n",
       "Layer (type:depth-idx)                        Param #\n",
       "======================================================================\n",
       "GPT                                           2,048\n",
       "├─Embedding: 1-1                              1,536\n",
       "├─ModuleList: 1-2                             --\n",
       "│    └─TransformerBlock: 2-1                  --\n",
       "│    │    └─RMSNorm: 3-1                      16\n",
       "│    │    └─GroupedQueryAttention: 3-2        680\n",
       "│    │    └─RMSNorm: 3-3                      16\n",
       "│    │    └─MixtureOfExperts: 3-4             8,704\n",
       "│    └─TransformerBlock: 2-2                  --\n",
       "│    │    └─RMSNorm: 3-5                      16\n",
       "│    │    └─GroupedQueryAttention: 3-6        680\n",
       "│    │    └─RMSNorm: 3-7                      16\n",
       "│    │    └─MixtureOfExperts: 3-8             8,704\n",
       "│    └─TransformerBlock: 2-3                  --\n",
       "│    │    └─RMSNorm: 3-9                      16\n",
       "│    │    └─GroupedQueryAttention: 3-10       680\n",
       "│    │    └─RMSNorm: 3-11                     16\n",
       "│    │    └─MixtureOfExperts: 3-12            8,704\n",
       "│    └─TransformerBlock: 2-4                  --\n",
       "│    │    └─RMSNorm: 3-13                     16\n",
       "│    │    └─GroupedQueryAttention: 3-14       680\n",
       "│    │    └─RMSNorm: 3-15                     16\n",
       "│    │    └─MixtureOfExperts: 3-16            8,704\n",
       "│    └─TransformerBlock: 2-5                  --\n",
       "│    │    └─RMSNorm: 3-17                     16\n",
       "│    │    └─GroupedQueryAttention: 3-18       680\n",
       "│    │    └─RMSNorm: 3-19                     16\n",
       "│    │    └─MixtureOfExperts: 3-20            8,704\n",
       "│    └─TransformerBlock: 2-6                  --\n",
       "│    │    └─RMSNorm: 3-21                     16\n",
       "│    │    └─GroupedQueryAttention: 3-22       680\n",
       "│    │    └─RMSNorm: 3-23                     16\n",
       "│    │    └─MixtureOfExperts: 3-24            8,704\n",
       "├─LayerNorm: 1-3                              32\n",
       "├─Linear: 1-4                                 1,632\n",
       "======================================================================\n",
       "Total params: 61,744\n",
       "Trainable params: 61,744\n",
       "Non-trainable params: 0\n",
       "======================================================================"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from omni.models.gpt import GPT\n",
    "\n",
    "max_seq_length = 128\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = GPT(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=16,\n",
    "    n_heads=4,\n",
    "    n_kv_heads=1,\n",
    "    m=2,\n",
    "    num_layers=6,\n",
    "    max_seq_length=max_seq_length,\n",
    "    num_experts=8,\n",
    "    top_k=2,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=8e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d64c659a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   3%|▎         | 11/358 [00:12<06:24,  1.11s/it, loss=4.31]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     21\u001b[39m logits = model(x_batch)\n\u001b[32m     22\u001b[39m loss = criterion(logits.view(-\u001b[32m1\u001b[39m, vocab_size), y_batch.view(-\u001b[32m1\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m optimizer.step()\n\u001b[32m     26\u001b[39m pbar.set_postfix({\u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m: loss.item()})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/projects/omni/.venv/lib/python3.13/site-packages/torch/_tensor.py:630\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    620\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    621\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    622\u001b[39m         Tensor.backward,\n\u001b[32m    623\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    628\u001b[39m         inputs=inputs,\n\u001b[32m    629\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/projects/omni/.venv/lib/python3.13/site-packages/torch/autograd/__init__.py:364\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    359\u001b[39m     retain_graph = create_graph\n\u001b[32m    361\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    362\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    363\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/projects/omni/.venv/lib/python3.13/site-packages/torch/autograd/graph.py:865\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    863\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    864\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m865\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    866\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    869\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def get_batch(tokens, batch_size, seq_length):\n",
    "    start_indices = torch.randint(0, len(tokens) - seq_length - 1, (batch_size,))\n",
    "    x_batch = torch.stack([tokens[i : i + seq_length] for i in start_indices])\n",
    "    y_batch = torch.stack([tokens[i + 1 : i + seq_length + 1] for i in start_indices])\n",
    "    return x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "\n",
    "num_epochs = 5\n",
    "batch_size = 128\n",
    "step = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    num_batches = len(train_tokens) // (batch_size * max_seq_length)\n",
    "\n",
    "    pbar = trange(num_batches, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    for _ in pbar:\n",
    "        x_batch, y_batch = get_batch(train_tokens, batch_size, max_seq_length)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x_batch)\n",
    "        loss = criterion(logits.view(-1, vocab_size), y_batch.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pbar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        num_val_batches = len(val_tokens) // (batch_size * max_seq_length)\n",
    "        for _ in range(num_val_batches):\n",
    "            x_batch, y_batch = get_batch(val_tokens, batch_size, max_seq_length)\n",
    "            logits = model(x_batch)\n",
    "            loss = criterion(logits.view(-1, vocab_size), y_batch.view(-1))\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / num_val_batches\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1067a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def generate_text(model, tokenizer, prompt, max_length=24):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(prompt).unsqueeze(0).to(device)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        logits = model(input_ids)\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)\n",
    "        input_ids = torch.cat([input_ids, next_token_id], dim=1)\n",
    "\n",
    "    return tokenizer.decode(input_ids.squeeze())\n",
    "\n",
    "\n",
    "print(generate_text(model, tokenizer, \"First\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccfd8a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
