{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaec108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch import nn\n",
    "from torchinfo import summary\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8290a75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"noanabeshima/TinyStoriesV2\")\n",
    "\n",
    "\n",
    "train_data = \"\\n\\n\".join(ds[\"train\"][\"text\"])\n",
    "val_data = \"\\n\\n\".join(ds[\"validation\"][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca977554",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(\n",
    "        self, text: str, special_tokens: list[str] = [\"<UNK>\", \"<SOS>\", \"<EOS>\"]\n",
    "    ):\n",
    "        chars = set(text)\n",
    "        self.char2idx = {\n",
    "            char: idx for idx, char in enumerate(special_tokens + sorted(chars))\n",
    "        }\n",
    "        self.idx2char = {idx: char for char, idx in self.char2idx.items()}\n",
    "\n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        return [self.char2idx.get(char, self.char2idx[\"<UNK>\"]) for char in text]\n",
    "\n",
    "    def decode(self, tokens: list[int]) -> str:\n",
    "        return \"\".join(self.idx2char.get(token, \"<UNK>\") for token in tokens)\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(\n",
    "    train_data,\n",
    ")\n",
    "vocab_size = len(tokenizer.char2idx)\n",
    "train_tokens = tokenizer.encode(train_data)\n",
    "val_tokens = tokenizer.encode(val_data)\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Train tokens: {len(train_tokens):,}\")\n",
    "print(f\"Validation tokens: {len(val_tokens):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a24a5394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roundTo32(x, base=32):\n",
    "    return int(base * math.ceil(float(x) / base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01575c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "======================================================================\n",
       "Layer (type:depth-idx)                        Param #\n",
       "======================================================================\n",
       "GPT                                           1,024\n",
       "├─Embedding: 1-1                              3,072\n",
       "├─ModuleList: 1-2                             --\n",
       "│    └─TransformerBlock: 2-1                  --\n",
       "│    │    └─RMSNorm: 3-1                      32\n",
       "│    │    └─GroupedQueryAttention: 3-2        2,640\n",
       "│    │    └─RMSNorm: 3-3                      32\n",
       "│    │    └─MixtureOfExperts: 3-4             67,584\n",
       "│    └─TransformerBlock: 2-2                  --\n",
       "│    │    └─RMSNorm: 3-5                      32\n",
       "│    │    └─GroupedQueryAttention: 3-6        2,640\n",
       "│    │    └─RMSNorm: 3-7                      32\n",
       "│    │    └─MixtureOfExperts: 3-8             67,584\n",
       "│    └─TransformerBlock: 2-3                  --\n",
       "│    │    └─RMSNorm: 3-9                      32\n",
       "│    │    └─GroupedQueryAttention: 3-10       2,640\n",
       "│    │    └─RMSNorm: 3-11                     32\n",
       "│    │    └─MixtureOfExperts: 3-12            67,584\n",
       "│    └─TransformerBlock: 2-4                  --\n",
       "│    │    └─RMSNorm: 3-13                     32\n",
       "│    │    └─GroupedQueryAttention: 3-14       2,640\n",
       "│    │    └─RMSNorm: 3-15                     32\n",
       "│    │    └─MixtureOfExperts: 3-16            67,584\n",
       "├─LayerNorm: 1-3                              64\n",
       "├─Linear: 1-4                                 3,168\n",
       "======================================================================\n",
       "Total params: 288,480\n",
       "Trainable params: 288,480\n",
       "Non-trainable params: 0\n",
       "======================================================================"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from omni.models.gpt import GPT\n",
    "\n",
    "max_seq_length = 128\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "model = GPT(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=32,\n",
    "    n_heads=8,\n",
    "    n_kv_heads=2,\n",
    "    m=2,\n",
    "    num_layers=4,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tie_weights=True,\n",
    "    num_experts=16,\n",
    "    top_k=1,\n",
    ").to(device)\n",
    "\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-1, momentum=0.99)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=8e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c1067a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there was and the the the the the the the the the the the the the the the the the the ther the the wa there the\n"
     ]
    }
   ],
   "source": [
    "def greedy(logits):\n",
    "    return torch.argmax(logits, dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "def sample(logits, temperature=1.0):\n",
    "    if temperature == 0:\n",
    "        return greedy(logits)\n",
    "    else:\n",
    "        probs = torch.softmax(logits / temperature, dim=-1)\n",
    "        return torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate_text(model, tokenizer, prompt, max_length=100, sample=greedy):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(prompt).unsqueeze(0).to(device)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        logits = model(input_ids)\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        next_token_id = sample(next_token_logits)\n",
    "\n",
    "        input_ids = torch.cat([input_ids, next_token_id], dim=1)\n",
    "\n",
    "    return tokenizer.decode(input_ids.squeeze())\n",
    "\n",
    "\n",
    "print(generate_text(model, tokenizer, \"Once upon a time, there was a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d64c659a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 720/720 [01:32<00:00,  7.77it/s, loss=1.95]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.9777\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'generate_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     38\u001b[39m avg_val_loss = val_loss / num_val_batches\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mValidation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_val_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mgenerate_text\u001b[49m(model, tokenizer, \u001b[33m\"\u001b[39m\u001b[33mFirst Citizen\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[31mNameError\u001b[39m: name 'generate_text' is not defined"
     ]
    }
   ],
   "source": [
    "def get_batch(tokens, batch_size, seq_length):\n",
    "    start_indices = torch.randint(0, len(tokens) - seq_length - 1, (batch_size,))\n",
    "    x_batch = torch.stack([tokens[i : i + seq_length] for i in start_indices])\n",
    "    y_batch = torch.stack([tokens[i + 1 : i + seq_length + 1] for i in start_indices])\n",
    "    return x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "\n",
    "num_epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    num_batches = len(train_tokens) // (batch_size * max_seq_length)\n",
    "\n",
    "    pbar = trange(num_batches, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    for _ in pbar:\n",
    "        x_batch, y_batch = get_batch(train_tokens, batch_size, max_seq_length)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x_batch)\n",
    "        loss = criterion(logits.view(-1, vocab_size), y_batch.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pbar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        num_val_batches = len(val_tokens) // (batch_size * max_seq_length)\n",
    "        for _ in range(num_val_batches):\n",
    "            x_batch, y_batch = get_batch(val_tokens, batch_size, max_seq_length)\n",
    "            logits = model(x_batch)\n",
    "            loss = criterion(logits.view(-1, vocab_size), y_batch.view(-1))\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / num_val_batches\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "        print(generate_text(model, tokenizer, \"First Citizen\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccfd8a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
