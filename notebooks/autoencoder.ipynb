{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa5d650f",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235e158b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from omni.utils.dataset import TensorImageFolder\n",
    "from omni.utils.device import get_device\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c73568",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()\n",
    "use_bf16 = device.type == \"cuda\" and torch.cuda.is_bf16_supported()\n",
    "\n",
    "train_data = TensorImageFolder(\"../data/afhq_v2_preprocessed/train\")\n",
    "test_data = TensorImageFolder(\"../data/afhq_v2_preprocessed/test\")\n",
    "\n",
    "cpu_count = os.cpu_count() or 1\n",
    "num_workers = min(8, cpu_count // 2)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=64,  # you can go higher now\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=512,  # you can go higher now\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133f7142",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int, groups: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm1 = nn.GroupNorm(groups, in_ch)\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1)\n",
    "\n",
    "        self.norm2 = nn.GroupNorm(groups, out_ch)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1)\n",
    "\n",
    "        self.skip = (\n",
    "            nn.Identity()\n",
    "            if in_ch == out_ch\n",
    "            else nn.Conv2d(in_ch, out_ch, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.conv1(F.silu(self.norm1(x)))\n",
    "        h = self.conv2(F.silu(self.norm2(h)))\n",
    "        return h + self.skip(x)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        base_channels: int,\n",
    "        latent_channels: int,\n",
    "        groups: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_in = nn.Conv2d(in_channels, base_channels, 3, padding=1)\n",
    "\n",
    "        self.down1 = nn.Sequential(\n",
    "            ResBlock(base_channels, base_channels, groups),\n",
    "            nn.Conv2d(base_channels, base_channels, 4, stride=2, padding=1),\n",
    "        )\n",
    "\n",
    "        self.down2 = nn.Sequential(\n",
    "            ResBlock(base_channels, base_channels * 2, groups),\n",
    "            nn.Conv2d(base_channels * 2, base_channels * 2, 4, stride=2, padding=1),\n",
    "        )\n",
    "\n",
    "        self.down3 = nn.Sequential(\n",
    "            ResBlock(base_channels * 2, base_channels * 4, groups),\n",
    "            nn.Conv2d(base_channels * 4, base_channels * 4, 4, stride=2, padding=1),\n",
    "        )\n",
    "\n",
    "        self.mid = ResBlock(base_channels * 4, base_channels * 4, groups)\n",
    "\n",
    "        self.norm_out = nn.GroupNorm(groups, base_channels * 4)\n",
    "        self.conv_out = nn.Conv2d(base_channels * 4, latent_channels, 3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_in(x)\n",
    "        x = self.down1(x)\n",
    "        x = self.down2(x)\n",
    "        x = self.down3(x)\n",
    "        x = self.mid(x)\n",
    "        x = self.conv_out(F.silu(self.norm_out(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        out_channels: int,\n",
    "        base_channels: int,\n",
    "        latent_channels: int,\n",
    "        groups: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_in = nn.Conv2d(latent_channels, base_channels * 4, 3, padding=1)\n",
    "\n",
    "        self.mid = ResBlock(base_channels * 4, base_channels * 4, groups)\n",
    "\n",
    "        self.up1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "            ResBlock(base_channels * 4, base_channels * 2, groups),\n",
    "        )\n",
    "\n",
    "        self.up2 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "            ResBlock(base_channels * 2, base_channels, groups),\n",
    "        )\n",
    "\n",
    "        self.up3 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "            ResBlock(base_channels, base_channels, groups),\n",
    "        )\n",
    "\n",
    "        self.norm_out = nn.GroupNorm(groups, base_channels)\n",
    "        self.conv_out = nn.Conv2d(base_channels, out_channels, 3, padding=1)\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.conv_in(z)\n",
    "        x = self.mid(x)\n",
    "        x = self.up1(x)\n",
    "        x = self.up2(x)\n",
    "        x = self.up3(x)\n",
    "        x = self.conv_out(F.silu(self.norm_out(x)))\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    SDXL-style spatial autoencoder.\n",
    "\n",
    "    Defaults:\n",
    "      - 512x512 images\n",
    "      - Latents: 4 x 64 x 64\n",
    "      - GroupNorm + residual blocks\n",
    "      - SD-compatible latent scaling\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_channels: int = 3,\n",
    "        base_channels: int = 32,\n",
    "        latent_channels: int = 4,\n",
    "        groupnorm_groups: int = 32,\n",
    "        latent_scale: float = 0.18215,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.latent_channels = latent_channels\n",
    "        self.latent_scale = latent_scale\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            in_channels=image_channels,\n",
    "            base_channels=base_channels,\n",
    "            latent_channels=latent_channels,\n",
    "            groups=groupnorm_groups,\n",
    "        )\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            out_channels=image_channels,\n",
    "            base_channels=base_channels,\n",
    "            latent_channels=latent_channels,\n",
    "            groups=groupnorm_groups,\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder(x) * self.latent_scale\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z / self.latent_scale)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        return self.decode(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26367db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.functional.image import (\n",
    "    peak_signal_noise_ratio,\n",
    "    structural_similarity_index_measure,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa995f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "\n",
    "model = Autoencoder().to(device)\n",
    "\n",
    "optimizer_ae = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "criterion_recon = nn.MSELoss()\n",
    "\n",
    "scaler = None  # not used for bf16\n",
    "\n",
    "print(summary(model, (1, 3, 128, 128), device=device.type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9033b664",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}\")\n",
    "\n",
    "    for batch_idx, (images, _) in enumerate(pbar):\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        batch_size = images.size(0)\n",
    "\n",
    "        with torch.autocast(\n",
    "            device_type=\"cuda\",\n",
    "            dtype=torch.bfloat16,\n",
    "            enabled=use_bf16,\n",
    "        ):\n",
    "            images = images.float().div_(255.0)\n",
    "            recon = model(images)\n",
    "\n",
    "            # ==================\n",
    "            # Autoencoder step\n",
    "            # ==================\n",
    "            optimizer_ae.zero_grad(set_to_none=True)\n",
    "\n",
    "            recon = model(images)\n",
    "\n",
    "            # Reconstruction loss\n",
    "            loss_recon = criterion_recon(recon, images)\n",
    "\n",
    "            loss_ae = loss_recon\n",
    "            loss_ae.backward()\n",
    "            optimizer_ae.step()\n",
    "\n",
    "        step += 1\n",
    "        pbar.set_postfix(\n",
    "            {\n",
    "                \"loss_ae\": loss_ae.item(),\n",
    "                \"loss_recon\": loss_recon.item(),\n",
    "                \"psnr\": peak_signal_noise_ratio(recon, images, data_range=1.0).item(),\n",
    "                \"ssim\": structural_similarity_index_measure(\n",
    "                    recon, images, data_range=1.0\n",
    "                ).item(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # --------------------\n",
    "    # Eval\n",
    "    # --------------------\n",
    "    model.eval()\n",
    "\n",
    "    total_loss_recon = 0\n",
    "\n",
    "    with (\n",
    "        torch.no_grad(),\n",
    "        torch.autocast(\n",
    "            device_type=\"cuda\",\n",
    "            dtype=torch.bfloat16,\n",
    "            enabled=use_bf16,\n",
    "        ),\n",
    "    ):\n",
    "        for images, _ in test_loader:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            images = images.float().div_(255.0)\n",
    "            recon = model(images)\n",
    "\n",
    "            loss_recon = criterion_recon(recon, images)\n",
    "\n",
    "            total_loss_recon += loss_recon.item() * images.size(0)\n",
    "\n",
    "    print(f\"Test loss (recon): {total_loss_recon / len(test_loader.dataset):.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f7c36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with torch.no_grad():\n",
    "    images, _ = next(iter(test_loader))\n",
    "    images = images[:32].to(device, non_blocking=True)\n",
    "    images = images.float().div_(255.0)\n",
    "    recon = model(images)\n",
    "\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "grid = make_grid(torch.cat([images.cpu(), recon.cpu()], dim=0), nrow=16)\n",
    "plt.figure(figsize=(24, 16))\n",
    "plt.imshow(grid.permute(1, 2, 0))\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edd414c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.min(), grid.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000bf53a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
